#!/bin/bash

# ==============================================================================
# üöÄ ZENTRIA MANAGER - AUTO INSTALLER (ZERO-TO-HERO)
# ==============================================================================
# Instala√ß√£o completa e automatizada para VPS Ubuntu 20.04+
# Configura SWAP, Docker, Evolution API e Zentria.
# ==============================================================================

# Cores para output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

# Fun√ß√£o para tratamento de erros
error_exit() {
    echo -e "${RED}‚ùå Erro: $1${NC}" >&2
    exit 1
}

# Fun√ß√£o para avisos
warning() {
    echo -e "${YELLOW}‚ö†Ô∏è  $1${NC}"
}

# Fun√ß√£o para sucesso
success() {
    echo -e "${GREEN}‚úÖ $1${NC}"
}

# Fun√ß√£o para info
info() {
    echo -e "${BLUE}‚ÑπÔ∏è  $1${NC}"
}

# Salvar diret√≥rio atual
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"

# Mudar para o diret√≥rio raiz do projeto
cd "$PROJECT_ROOT" || error_exit "N√£o foi poss√≠vel acessar o diret√≥rio raiz do projeto"

echo -e "${BLUE}========================================${NC}"
echo -e "${BLUE}  ZENTRIA MANAGER - AUTO INSTALLER${NC}"
echo -e "${BLUE}========================================${NC}"
echo ""

# ==============================================================================
# [1/9] Atualizando Sistema
# ==============================================================================
echo -e "${BLUE}[1/9] Atualizando Sistema...${NC}"
export DEBIAN_FRONTEND=noninteractive
sudo apt update -qq || error_exit "Falha ao atualizar lista de pacotes"
sudo apt install -y curl git nano unzip ufw || error_exit "Falha ao instalar pacotes b√°sicos"
success "Sistema atualizado"

# ==============================================================================
# [2/9] Configurando SWAP (4GB)
# ==============================================================================
echo -e "${BLUE}[2/9] Configurando SWAP (4GB)...${NC}"
if [ $(swapon --show 2>/dev/null | wc -l) -eq 0 ]; then
    if [ ! -f /swapfile ]; then
        sudo fallocate -l 4G /swapfile || sudo dd if=/dev/zero of=/swapfile bs=1M count=4096
        sudo chmod 600 /swapfile
        sudo mkswap /swapfile
        sudo swapon /swapfile
        if ! grep -q "/swapfile" /etc/fstab; then
            echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab > /dev/null
        fi
        success "SWAP de 4GB configurado"
    else
        info "SWAP j√° existe, pulando configura√ß√£o"
    fi
else
    info "SWAP j√° est√° ativo"
fi

# ==============================================================================
# [3/9] Instalando Docker
# ==============================================================================
echo -e "${BLUE}[3/9] Instalando Docker...${NC}"
if ! command -v docker &> /dev/null; then
    curl -fsSL https://get.docker.com | sh || error_exit "Falha ao instalar Docker"
    sudo usermod -aG docker $USER 2>/dev/null || true
    # Tentar usar docker sem sudo (pode n√£o funcionar at√© logout/login)
    if ! sudo docker ps &> /dev/null; then
        warning "Docker instalado, mas pode ser necess√°rio logout/login para usar sem sudo"
    fi
    success "Docker instalado"
else
    info "Docker j√° est√° instalado"
fi

# Verificar se docker compose est√° dispon√≠vel
if ! command -v docker-compose &> /dev/null && ! docker compose version &> /dev/null; then
    warning "Docker Compose n√£o encontrado, tentando instalar..."
    sudo apt install -y docker-compose-plugin || error_exit "Falha ao instalar Docker Compose"
fi

# ==============================================================================
# [4/9] Instalando Node.js & PM2
# ==============================================================================
echo -e "${BLUE}[4/9] Instalando Node.js & PM2...${NC}"
if ! command -v node &> /dev/null; then
    curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash - || error_exit "Falha ao configurar reposit√≥rio Node.js"
    sudo apt install -y nodejs || error_exit "Falha ao instalar Node.js"
    success "Node.js instalado"
else
    NODE_VERSION=$(node -v)
    info "Node.js j√° est√° instalado: $NODE_VERSION"
fi

if ! command -v pm2 &> /dev/null; then
    sudo npm install -g pm2 serve || error_exit "Falha ao instalar PM2 e serve"
    success "PM2 e serve instalados"
else
    info "PM2 j√° est√° instalado"
fi

# ==============================================================================
# [5/9] Configurando Evolution API
# ==============================================================================
echo -e "${BLUE}[5/9] Configurando Evolution API...${NC}"

# Detectar IP do servidor automaticamente com fallbacks
SERVER_IP=$(hostname -I | awk '{print $1}' 2>/dev/null)
if [ -z "$SERVER_IP" ]; then
    SERVER_IP=$(ip addr show | grep "inet " | grep -v 127.0.0.1 | head -1 | awk '{print $2}' | cut -d'/' -f1 2>/dev/null)
fi
if [ -z "$SERVER_IP" ]; then
    SERVER_IP=$(ifconfig 2>/dev/null | grep "inet " | grep -v 127.0.0.1 | head -1 | awk '{print $2}' 2>/dev/null)
fi
if [ -z "$SERVER_IP" ]; then
    SERVER_IP="localhost"
    warning "N√£o foi poss√≠vel detectar o IP do servidor. Usando 'localhost'."
else
    success "IP do servidor detectado: ${SERVER_IP}"
fi

# Parar e remover containers existentes se houver
info "Verificando containers existentes..."

# Lista de containers que precisam ser removidos
CONTAINER_NAMES="evolution_postgres evolution_redis evolution_api"

# Fun√ß√£o para remover container por nome
remove_container() {
    local container_name=$1
    if sudo docker ps -a --format '{{.Names}}' 2>/dev/null | grep -q "^${container_name}$"; then
        info "Removendo container: $container_name"
        sudo docker stop "$container_name" 2>/dev/null || true
        sudo docker rm -f "$container_name" 2>/dev/null || true
    fi
}

# Primeiro, tentar usar docker compose down se o arquivo existir
if [ -f "docker-compose.yml" ]; then
    info "Tentando parar containers usando docker compose..."
    sudo docker compose down --remove-orphans 2>/dev/null || sudo docker-compose down --remove-orphans 2>/dev/null || true
    sleep 2
fi

# Remover containers manualmente por nome (garantir que todos sejam removidos)
info "Removendo containers manualmente por nome..."
for container_name in $CONTAINER_NAMES; do
    remove_container "$container_name"
done

# Verificar se ainda h√° containers √≥rf√£os com esses nomes
EXISTING_CONTAINERS=$(sudo docker ps -a --format '{{.Names}}' 2>/dev/null | grep -E "evolution_postgres|evolution_redis|evolution_api" || true)

if [ -n "$EXISTING_CONTAINERS" ]; then
    warning "Ainda h√° containers existentes. For√ßando remo√ß√£o..."
    echo "$EXISTING_CONTAINERS" | while read container; do
        if [ -n "$container" ]; then
            info "For√ßando remo√ß√£o de: $container"
            sudo docker stop "$container" 2>/dev/null || true
            sudo docker rm -f "$container" 2>/dev/null || true
        fi
    done
    sleep 2
fi

# Verifica√ß√£o final
REMAINING=$(sudo docker ps -a --format '{{.Names}}' 2>/dev/null | grep -E "evolution_postgres|evolution_redis|evolution_api" || true)
if [ -z "$REMAINING" ]; then
    success "Todos os containers antigos foram removidos"
else
    warning "Ainda h√° containers restantes: $REMAINING"
    warning "Tentando remover novamente..."
    echo "$REMAINING" | while read container; do
        if [ -n "$container" ]; then
            sudo docker rm -f "$container" 2>/dev/null || true
        fi
    done
    sleep 2
fi

# Criar docker-compose.yml
cat > docker-compose.yml <<EOL
services:
  evolution_api:
    image: evoapicloud/evolution-api:v2.3.4
    container_name: evolution_api
    restart: always
    shm_size: '2gb'
    dns:
      - 8.8.8.8
      - 8.8.4.4
    ports:
      - "8080:8080"
    environment:
      - SERVER_PORT=8080
      - SERVER_URL=http://${SERVER_IP}:8080
      - AUTHENTICATION_API_KEY=B8349283-F143-429D-B6C2-9386E8016558
      - WEBSOCKET_ENABLED=true
      - DATABASE_ENABLED=true
      - DATABASE_PROVIDER=postgresql
      - DATABASE_CONNECTION_URI=postgresql://user:password@evolution_postgres:5432/evolution
      - DATABASE_CLIENT_NAME=evolution_exchange
      - RABBITMQ_ENABLED=false
      - CACHE_REDIS_ENABLED=true
      - CACHE_REDIS_URI=redis://evolution_redis:6379/0
      - DEL_INSTANCE=false
      - STORE_MESSAGES=true
      - STORE_MESSAGE_UP=true
      - STORE_CONTACTS=true
      - STORE_CHATS=true
      - CONFIG_SESSION_PHONE_CLIENT=Zentria
      - CONFIG_SESSION_PHONE_NAME=Chrome
      - CONFIG_SESSION_PHONE_OS=Windows
      - CONFIG_SESSION_PHONE_SYNC_FULL_HISTORY=false
      - BROWSER_ARGS=["--no-sandbox","--disable-setuid-sandbox","--disable-dev-shm-usage","--disable-accelerated-2d-canvas","--no-first-run","--disable-gpu","--disable-software-rasterizer"]
      - CORS_ORIGIN=*
      - CORS_METHODS=POST,GET,PUT,DELETE,OPTIONS
      - CORS_CREDENTIALS=true
    depends_on:
      evolution_postgres:
        condition: service_healthy
      evolution_redis:
        condition: service_started

  evolution_postgres:
    image: postgres:15-alpine
    container_name: evolution_postgres
    restart: always
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=evolution
    volumes:
      - evolution_postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user -d evolution || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  evolution_redis:
    image: redis:alpine
    container_name: evolution_redis
    restart: always
    command: ["redis-server", "--appendonly", "yes"]
    volumes:
      - evolution_redis_data:/data

volumes:
  evolution_postgres_data:
  evolution_redis_data:
EOL

# Verificar se o arquivo foi criado corretamente
if [ ! -f "docker-compose.yml" ]; then
    error_exit "Falha ao criar docker-compose.yml"
fi

# Validar sintaxe do docker-compose.yml (se poss√≠vel)
info "Validando docker-compose.yml..."
if sudo docker compose config &> /dev/null || sudo docker-compose config &> /dev/null; then
    success "docker-compose.yml √© v√°lido"
else
    warning "N√£o foi poss√≠vel validar docker-compose.yml, mas continuando..."
fi

# Verifica√ß√£o final: garantir que n√£o h√° containers com os nomes que vamos usar
info "Verifica√ß√£o final: garantindo que n√£o h√° conflitos de nomes..."
CONFLICT_CONTAINERS=$(sudo docker ps -a --format '{{.Names}}' 2>/dev/null | grep -E "^evolution_postgres$|^evolution_redis$|^evolution_api$" || true)
if [ -n "$CONFLICT_CONTAINERS" ]; then
    warning "Ainda h√° containers com nomes conflitantes. Removendo..."
    echo "$CONFLICT_CONTAINERS" | while read container; do
        if [ -n "$container" ]; then
            info "Removendo container conflitante: $container"
            sudo docker stop "$container" 2>/dev/null || true
            sudo docker rm -f "$container" 2>/dev/null || true
        fi
    done
    sleep 2
    
    # Verificar novamente
    CONFLICT_CONTAINERS=$(sudo docker ps -a --format '{{.Names}}' 2>/dev/null | grep -E "^evolution_postgres$|^evolution_redis$|^evolution_api$" || true)
    if [ -n "$CONFLICT_CONTAINERS" ]; then
        error_exit "N√£o foi poss√≠vel remover containers conflitantes: $CONFLICT_CONTAINERS. Remova manualmente: sudo docker rm -f $CONFLICT_CONTAINERS"
    fi
fi
success "Nenhum conflito de nomes detectado"

# Subir containers
info "Iniciando containers Docker..."

# Verificar se Docker est√° rodando
if ! sudo docker ps &> /dev/null; then
    warning "Docker n√£o est√° respondendo. Tentando iniciar servi√ßo Docker..."
    sudo systemctl start docker 2>/dev/null || true
    sleep 2
    if ! sudo docker ps &> /dev/null; then
        error_exit "Docker n√£o est√° rodando. Execute: sudo systemctl start docker"
    fi
fi

# Verificar se docker compose est√° dispon√≠vel
DOCKER_COMPOSE_CMD=""
if sudo docker compose version &> /dev/null 2>&1; then
    DOCKER_COMPOSE_CMD="sudo docker compose"
    info "Usando: docker compose (v2)"
elif command -v docker-compose &> /dev/null && sudo docker-compose version &> /dev/null 2>&1; then
    DOCKER_COMPOSE_CMD="sudo docker-compose"
    info "Usando: docker-compose (v1)"
else
    # Tentar instalar docker-compose-plugin
    warning "Docker Compose n√£o encontrado. Tentando instalar..."
    sudo apt install -y docker-compose-plugin 2>/dev/null || sudo apt install -y docker-compose 2>/dev/null || true
    sleep 2
    
    if sudo docker compose version &> /dev/null 2>&1; then
        DOCKER_COMPOSE_CMD="sudo docker compose"
        info "Docker Compose instalado e usando: docker compose (v2)"
    elif sudo docker-compose version &> /dev/null 2>&1; then
        DOCKER_COMPOSE_CMD="sudo docker-compose"
        info "Docker Compose instalado e usando: docker-compose (v1)"
    else
        error_exit "Docker Compose n√£o est√° dispon√≠vel. Instale manualmente: sudo apt install docker-compose-plugin"
    fi
fi

# Tentar iniciar containers e capturar erros
info "Executando: $DOCKER_COMPOSE_CMD up -d"

# Executar e capturar sa√≠da
TEMP_OUTPUT=$(mktemp)
if $DOCKER_COMPOSE_CMD up -d > "$TEMP_OUTPUT" 2>&1; then
    DOCKER_EXIT_CODE=0
    DOCKER_OUTPUT=$(cat "$TEMP_OUTPUT")
    rm -f "$TEMP_OUTPUT"
else
    DOCKER_EXIT_CODE=$?
    DOCKER_OUTPUT=$(cat "$TEMP_OUTPUT")
    rm -f "$TEMP_OUTPUT"
fi

if [ $DOCKER_EXIT_CODE -eq 0 ]; then
    success "Containers Docker iniciados"
    # Aguardar containers iniciarem
    info "Aguardando containers iniciarem..."
    sleep 5
    
    # Verificar status dos containers
    info "Verificando status dos containers..."
    if $DOCKER_COMPOSE_CMD ps | grep -q "Up"; then
        success "Containers est√£o rodando"
    else
        warning "Alguns containers podem n√£o estar rodando. Verificando..."
        $DOCKER_COMPOSE_CMD ps
    fi
else
    echo -e "${RED}Erro ao iniciar containers Docker:${NC}"
    echo "$DOCKER_OUTPUT"
    echo ""
    
    # Tentar diagnosticar o problema
    warning "Tentando diagnosticar o problema..."
    
    # Verificar se as portas est√£o em uso
    if sudo netstat -tuln 2>/dev/null | grep -q ":8080 "; then
        warning "Porta 8080 j√° est√° em uso"
    fi
    
    # Verificar se h√° problemas com imagens
    info "Verificando imagens Docker..."
    sudo docker images | grep -E "evolution-api|postgres|redis" || warning "Imagens n√£o encontradas localmente"
    
    # Tentar fazer pull das imagens primeiro
    info "Tentando baixar imagens necess√°rias..."
    sudo docker pull evoapicloud/evolution-api:v2.3.4 || warning "Falha ao baixar imagem evolution-api"
    sudo docker pull postgres:15-alpine || warning "Falha ao baixar imagem postgres"
    sudo docker pull redis:alpine || warning "Falha ao baixar imagem redis"
    
    # Remover containers conflitantes antes de tentar novamente
    info "Removendo containers conflitantes antes da segunda tentativa..."
    for container_name in evolution_postgres evolution_redis evolution_api; do
        if sudo docker ps -a --format '{{.Names}}' 2>/dev/null | grep -q "^${container_name}$"; then
            info "Removendo: $container_name"
            sudo docker stop "$container_name" 2>/dev/null || true
            sudo docker rm -f "$container_name" 2>/dev/null || true
        fi
    done
    sleep 2
    
    # Tentar novamente
    info "Tentando iniciar containers novamente..."
    TEMP_OUTPUT2=$(mktemp)
    if $DOCKER_COMPOSE_CMD up -d > "$TEMP_OUTPUT2" 2>&1; then
        success "Containers Docker iniciados na segunda tentativa"
        rm -f "$TEMP_OUTPUT2"
        sleep 5
    else
        echo -e "${RED}Erro na segunda tentativa:${NC}"
        cat "$TEMP_OUTPUT2"
        ERROR_CONTENT=$(cat "$TEMP_OUTPUT2")
        rm -f "$TEMP_OUTPUT2"
        
        # Verificar se o erro √© de conflito de nomes
        if echo "$ERROR_CONTENT" | grep -q "container name.*is already in use"; then
            warning "Detectado conflito de nomes de containers. Tentando remover containers √≥rf√£os..."
            
            # Extrair nomes de containers do erro e remover
            for container_name in evolution_postgres evolution_redis evolution_api; do
                CONTAINER_ID=$(sudo docker ps -a --filter "name=^${container_name}$" --format '{{.ID}}' 2>/dev/null || true)
                if [ -n "$CONTAINER_ID" ]; then
                    info "Removendo container √≥rf√£o: $container_name (ID: $CONTAINER_ID)"
                    sudo docker rm -f "$CONTAINER_ID" 2>/dev/null || true
                fi
            done
            
            sleep 2
            
            # Tentar uma √∫ltima vez
            info "Tentativa final ap√≥s remover containers √≥rf√£os..."
            TEMP_OUTPUT3=$(mktemp)
            if $DOCKER_COMPOSE_CMD up -d > "$TEMP_OUTPUT3" 2>&1; then
                success "Containers Docker iniciados na tentativa final"
                rm -f "$TEMP_OUTPUT3"
                sleep 5
            else
                echo -e "${RED}Erro na tentativa final:${NC}"
                cat "$TEMP_OUTPUT3"
                rm -f "$TEMP_OUTPUT3"
                error_exit "Falha ao iniciar containers Docker. Execute manualmente: sudo docker rm -f \$(sudo docker ps -a -q --filter 'name=evolution_')"
            fi
        else
            error_exit "Falha ao iniciar containers Docker mesmo ap√≥s baixar imagens. Verifique os logs acima."
        fi
    fi
fi

# ==============================================================================
# [6/9] Instalando PostgreSQL (para backend)
# ==============================================================================
echo -e "${BLUE}[6/9] Instalando PostgreSQL (para backend)...${NC}"
if ! command -v psql &> /dev/null; then
    sudo apt install -y postgresql postgresql-contrib || error_exit "Falha ao instalar PostgreSQL"
    sudo systemctl start postgresql || error_exit "Falha ao iniciar PostgreSQL"
    sudo systemctl enable postgresql || true
    success "PostgreSQL instalado e iniciado"
else
    info "PostgreSQL j√° est√° instalado"
    # Garantir que est√° rodando
    sudo systemctl start postgresql 2>/dev/null || true
fi

# Configurar PostgreSQL na porta alta (54321) para evitar conflitos
info "Configurando PostgreSQL na porta 54321..."
PG_CONF=$(sudo find /etc/postgresql -name postgresql.conf 2>/dev/null | head -1)
if [ -n "$PG_CONF" ]; then
    # Fazer backup
    sudo cp "$PG_CONF" "${PG_CONF}.backup" 2>/dev/null || true
    # Alterar porta para 54321
    if ! grep -q "^port = 54321" "$PG_CONF"; then
        sudo sed -i "s/^#*port = .*/port = 54321/" "$PG_CONF" 2>/dev/null || true
        if ! grep -q "^port = 54321" "$PG_CONF"; then
            echo "port = 54321" | sudo tee -a "$PG_CONF" > /dev/null
        fi
        sudo systemctl restart postgresql
        success "PostgreSQL configurado para usar porta 54321"
    else
        info "PostgreSQL j√° est√° configurado para porta 54321"
    fi
else
    warning "Arquivo de configura√ß√£o do PostgreSQL n√£o encontrado."
    warning "Configure manualmente: sudo nano /etc/postgresql/*/main/postgresql.conf"
fi

# Aguardar PostgreSQL iniciar completamente
info "Aguardando PostgreSQL iniciar..."
for i in {1..30}; do
    if sudo -u postgres PGPORT=54321 psql -c "SELECT 1;" > /dev/null 2>&1; then
        success "PostgreSQL est√° pronto"
        break
    fi
    if [ $i -eq 30 ]; then
        warning "PostgreSQL n√£o respondeu ap√≥s 30 tentativas, continuando mesmo assim..."
    fi
    sleep 1
done

# Configurar banco de dados (usando porta 54321)
info "Configurando banco de dados..."
sudo -u postgres PGPORT=54321 psql -c "CREATE DATABASE zentria;" 2>/dev/null || info "Banco zentria j√° existe"
sudo -u postgres PGPORT=54321 psql -c "CREATE USER zentria_user WITH PASSWORD 'zentria_secure_password_2024';" 2>/dev/null || info "Usu√°rio zentria_user j√° existe"
sudo -u postgres PGPORT=54321 psql -c "GRANT ALL PRIVILEGES ON DATABASE zentria TO zentria_user;" 2>/dev/null || true
sudo -u postgres PGPORT=54321 psql -c "ALTER USER zentria_user CREATEDB;" 2>/dev/null || true
success "Banco de dados configurado"

# ==============================================================================
# [7/9] Configurando Backend API
# ==============================================================================
echo -e "${BLUE}[7/9] Configurando Backend API...${NC}"

# Verificar se o diret√≥rio backend existe
if [ ! -d "backend" ]; then
    warning "Diret√≥rio backend n√£o encontrado. Criando estrutura b√°sica..."
    mkdir -p backend/scripts
fi

cd backend || error_exit "N√£o foi poss√≠vel acessar o diret√≥rio backend"

# Criar package.json se n√£o existir
if [ ! -f "package.json" ]; then
    info "Criando package.json do backend..."
    cat > package.json <<EOL
{
  "name": "zentria-backend",
  "version": "1.0.0",
  "description": "Backend API para Zentria com PostgreSQL",
  "type": "module",
  "main": "server.js",
  "scripts": {
    "dev": "node --watch server.js",
    "start": "node server.js",
    "build": "echo 'Backend n√£o requer build - Node.js puro' && exit 0",
    "migrate": "node scripts/migrate.js",
    "migrate-holidays-cache": "node scripts/add-municipal-holidays-cache.js",
    "migrate-gemini-quota": "node scripts/add-gemini-quota-control.js",
    "migrate-national-holidays": "node scripts/add-national-holidays-table.js",
    "create-admin": "node scripts/create-admin.js",
    "update-user-name": "node scripts/update-user-name.js",
    "validate-users": "node scripts/validate-users.js"
  },
  "dependencies": {
    "express": "^4.18.2",
    "pg": "^8.11.3",
    "cors": "^2.8.5",
    "dotenv": "^16.3.1",
    "bcryptjs": "^2.4.3",
    "jsonwebtoken": "^9.0.2",
    "express-rate-limit": "^7.1.5"
  }
}
EOL
fi

# Instalar depend√™ncias do backend
info "Instalando depend√™ncias do backend..."
if npm install --silent 2>&1 | grep -i "error\|fatal" > /dev/null; then
    warning "Alguns erros durante instala√ß√£o de depend√™ncias, mas continuando..."
else
    success "Depend√™ncias do backend instaladas"
fi

# Gerar JWT secret
JWT_SECRET=$(openssl rand -hex 32 2>/dev/null || head -c 32 /dev/urandom | base64 | tr -d '\n' || echo "default_secret_key_change_in_production_$(date +%s)")

# Criar arquivo .env do backend
info "Criando arquivo .env do backend..."
cat > .env <<EOL
# Configura√ß√£o do PostgreSQL (porta alta para evitar conflitos)
DATABASE_URL=postgresql://zentria_user:zentria_secure_password_2024@localhost:54321/zentria
DB_HOST=localhost
DB_PORT=54321
DB_NAME=zentria
DB_USER=zentria_user
DB_PASSWORD=zentria_secure_password_2024

# JWT Secret para autentica√ß√£o
JWT_SECRET=${JWT_SECRET}

# Porta do servidor
PORT=3001

# CORS - URLs permitidas
CORS_ORIGIN=http://${SERVER_IP}:5173,http://localhost:5173

# IP do servidor (opcional - backend tamb√©m tenta detectar automaticamente)
SERVER_IP=${SERVER_IP}

# ============================================================================
# Evolution API / Webhook (auto-configura√ß√£o)
# ============================================================================
# Fonte de verdade para o backend reaplicar webhook automaticamente no startup.
# Se voc√™ alterar a AUTHENTICATION_API_KEY no docker-compose, atualize aqui tamb√©m.
EVOLUTION_BASE_URL=http://${SERVER_IP}:8080
EVOLUTION_AUTH_KEY=B8349283-F143-429D-B6C2-9386E8016558
EVOLUTION_WEBHOOK_URL=http://${SERVER_IP}:3001/api/webhook/evolution
EVOLUTION_WEBHOOK_BY_EVENTS=true
EVOLUTION_WEBHOOK_BASE64=true
EVOLUTION_WEBHOOK_EVENTS=MESSAGES_UPSERT,MESSAGES_UPDATE,CHATS_UPSERT,CHATS_UPDATE,CONTACTS_UPSERT,CONNECTION_UPDATE
EOL

# Verificar se os arquivos do backend existem
if [ ! -f "server.js" ] || [ ! -f "scripts/migrate.js" ]; then
    warning "Arquivos do backend n√£o encontrados no diret√≥rio backend/"
    warning "Certifique-se de que voc√™ fez git clone completo do reposit√≥rio."
    warning "Continuando sem backend... (os dados ser√£o salvos apenas no localStorage)"
    BACKEND_AVAILABLE=false
else
    BACKEND_AVAILABLE=true
    mkdir -p scripts
    
    # Executar migra√ß√£o principal
    info "Executando migra√ß√£o principal do banco de dados..."
    if npm run migrate 2>&1; then
        success "Migra√ß√£o principal conclu√≠da"
        
        # Executar TODAS as migra√ß√µes adicionais automaticamente
        info "Executando migra√ß√µes adicionais do banco de dados..."
        
        # Migra√ß√£o: Cache de feriados municipais
        if [ -f "scripts/add-municipal-holidays-cache.js" ]; then
            npm run migrate-holidays-cache 2>/dev/null || true
        fi
        
        # Migra√ß√£o: Controle de cota do Gemini
        if [ -f "scripts/add-gemini-quota-control.js" ]; then
            npm run migrate-gemini-quota 2>/dev/null || true
        fi
        
        # Migra√ß√£o: Tabela de feriados nacionais
        if [ -f "scripts/add-national-holidays-table.js" ]; then
            npm run migrate-national-holidays 2>/dev/null || true
        fi
        
        # Adicionar campo department_id na tabela users (se n√£o existir)
        if [ -f "scripts/add-department-id-to-users.js" ]; then
            node scripts/add-department-id-to-users.js 2>/dev/null || true
        fi
        
        # Corrigir data_keys de chats (opcional)
        if [ -f "scripts/fix-chat-data-keys.js" ]; then
            node scripts/fix-chat-data-keys.js 2>/dev/null || true
        fi
        
        # Migrar configura√ß√µes para globais (user_id = NULL)
        if [ -f "scripts/migrate-config-to-global.js" ]; then
            node scripts/migrate-config-to-global.js 2>/dev/null || true
        fi
        
        # Limpar chats inv√°lidos (opcional, mas recomendado)
        if [ -f "scripts/clean-invalid-chats.js" ]; then
            node scripts/clean-invalid-chats.js 2>/dev/null || true
        fi
        
        # Criar usu√°rio admin padr√£o (se n√£o existir)
        if [ -f "scripts/create-admin.js" ]; then
            npm run create-admin 2>/dev/null || true
        fi
        
        success "Todas as migra√ß√µes do banco de dados foram executadas"
    else
        warning "Erro na migra√ß√£o principal. Verificando conex√£o com PostgreSQL..."
        BACKEND_AVAILABLE=false
    fi
fi

cd "$PROJECT_ROOT" || error_exit "N√£o foi poss√≠vel retornar ao diret√≥rio raiz"

# ==============================================================================
# [8/9] Configurando Frontend
# ==============================================================================
echo -e "${BLUE}[8/9] Configurando Frontend...${NC}"

# Zentria agora usa estrutura /frontend (monorepo)
if [ -d "frontend" ] && [ -f "frontend/package.json" ]; then
    cd frontend || error_exit "N√£o foi poss√≠vel acessar a pasta frontend"
else
    # Compat: estrutura antiga (frontend na raiz)
    if [ ! -f "package.json" ]; then
        error_exit "frontend/package.json n√£o encontrado. Certifique-se de que o reposit√≥rio foi clonado corretamente."
    fi
fi

# Configurar vari√°vel de ambiente do frontend para usar o backend
if [ "$BACKEND_AVAILABLE" = true ]; then
    if [ ! -f ".env" ]; then
        echo "VITE_API_URL=http://${SERVER_IP}:3001" > .env
        success "Arquivo .env do frontend criado"
    else
        # Adicionar ou atualizar VITE_API_URL no .env existente
        if grep -q "VITE_API_URL" .env; then
            sed -i "s|VITE_API_URL=.*|VITE_API_URL=http://${SERVER_IP}:3001|" .env
        else
            echo "VITE_API_URL=http://${SERVER_IP}:3001" >> .env
        fi
        success "Vari√°vel VITE_API_URL configurada no .env"
    fi
    info "VITE_API_URL=http://${SERVER_IP}:3001"
fi

# Instalar depend√™ncias do frontend
info "Instalando depend√™ncias do frontend..."
if [ -d "node_modules" ]; then
    rm -rf node_modules package-lock.json yarn.lock 2>/dev/null || true
fi

if npm install --silent 2>&1 | grep -i "error\|fatal" > /dev/null; then
    warning "Alguns erros durante instala√ß√£o de depend√™ncias, mas continuando..."
else
    success "Depend√™ncias do frontend instaladas"
fi

# Construir aplica√ß√£o frontend
info "Construindo aplica√ß√£o frontend..."
if npm run build 2>&1; then
    success "Build do frontend conclu√≠do"
else
    warning "Erro ao fazer build do frontend. Verifique os logs acima."
    warning "Continuando mesmo assim..."
fi

# ==============================================================================
# [9/9] Finalizando
# ==============================================================================
echo -e "${BLUE}[9/9] Finalizando...${NC}"

# Garante que estamos no diret√≥rio raiz do projeto antes de iniciar servi√ßos
cd "$PROJECT_ROOT" || error_exit "N√£o foi poss√≠vel retornar ao diret√≥rio raiz"

# Iniciar backend com PM2 (se os arquivos existirem e migra√ß√£o foi bem-sucedida)
if [ "$BACKEND_AVAILABLE" = true ] && [ -f "backend/server.js" ]; then
    pm2 delete zentria-backend 2>/dev/null || true
    cd backend
    if pm2 start server.js --name zentria-backend --update-env; then
        success "Backend iniciado com PM2"
        
        # Aguardar servidor iniciar
        info "Aguardando servidor backend iniciar..."
        sleep 5
        
        # Testar conectividade da API
        info "Testando conectividade da API backend..."
        API_URL="http://${SERVER_IP}:3001/api/health"
        
        # Tentar conectar at√© 15 vezes (15 segundos)
        API_ACCESSIBLE=false
        for i in {1..15}; do
            if curl -s -f -m 2 "$API_URL" > /dev/null 2>&1; then
                API_ACCESSIBLE=true
                success "API backend est√° acess√≠vel em $API_URL"
                break
            fi
            echo -n "."
            sleep 1
        done
        echo ""
        
        if [ "$API_ACCESSIBLE" = false ]; then
            warning "API backend n√£o est√° respondendo ap√≥s 15 segundos"
            warning "Isso pode ser normal se o servidor ainda est√° inicializando"
        fi
    else
        warning "Falha ao iniciar backend com PM2"
    fi
    cd "$PROJECT_ROOT"
else
    warning "Backend n√£o iniciado (arquivos n√£o encontrados ou erro na migra√ß√£o)"
    warning "Os dados ser√£o salvos apenas no localStorage do navegador."
fi

# Iniciar frontend com PM2
info "Iniciando frontend com PM2..."
pm2 delete zentria-front 2>/dev/null || true
cd "$PROJECT_ROOT/frontend" 2>/dev/null || cd "$PROJECT_ROOT" || true
if [ -d "dist" ]; then
    if pm2 start "serve -s dist -l 5173" --name zentria-front; then
        success "Frontend iniciado com PM2"
        pm2 save || true
        # Tentar configurar startup autom√°tico
        pm2 startup 2>/dev/null | grep "sudo" | bash 2>/dev/null || true
    else
        warning "Falha ao iniciar frontend com PM2"
    fi
else
    warning "Diret√≥rio dist n√£o encontrado. Frontend n√£o pode ser iniciado."
fi

# Configurar firewall
if command -v ufw &> /dev/null; then
    info "Configurando firewall..."
    echo "y" | sudo ufw allow 8080/tcp > /dev/null 2>&1  # Evolution API
    echo "y" | sudo ufw allow 5173/tcp > /dev/null 2>&1  # Frontend
    echo "y" | sudo ufw allow 3001/tcp > /dev/null 2>&1  # Backend API
    echo "y" | sudo ufw allow 22/tcp > /dev/null 2>&1    # SSH
    echo "y" | sudo ufw allow 80/tcp > /dev/null 2>&1    # HTTP
    echo "y" | sudo ufw --force enable > /dev/null 2>&1 || true
    success "Firewall configurado"
fi

# ==============================================================================
# Resumo Final
# ==============================================================================
echo ""
echo -e "${GREEN}========================================${NC}"
echo -e "${GREEN}  INSTALA√á√ÉO CONCLU√çDA COM SUCESSO!${NC}"
echo -e "${GREEN}========================================${NC}"
echo ""
echo -e "üåê Frontend: ${GREEN}http://${SERVER_IP}:5173${NC}"
echo -e "üîå Evolution API: ${GREEN}http://${SERVER_IP}:8080${NC}"
echo -e "üíæ Backend API: ${GREEN}http://${SERVER_IP}:3001${NC}"
echo ""
echo -e "${BLUE}Credenciais padr√£o do admin:${NC}"
echo -e "   Username: ${GREEN}admin@piekas.com${NC}"
echo -e "   Password: ${GREEN}123${NC}"
echo -e "   ${YELLOW}‚ö†Ô∏è  ALTERE A SENHA EM PRODU√á√ÉO!${NC}"
echo ""
echo -e "${BLUE}Pr√≥ximos passos ap√≥s login:${NC}"
echo -e "   1. Criar departamentos em ${YELLOW}Configura√ß√µes > Departamentos${NC}"
echo -e "   2. Criar usu√°rios e atribuir a departamentos em ${YELLOW}Configura√ß√µes > Usu√°rios${NC}"
echo -e "   3. Configurar Gemini API Key em ${YELLOW}Configura√ß√µes > Integra√ß√£o Google${NC} (opcional)"
echo -e "   4. Configurar Chatbot em ${YELLOW}Configura√ß√µes > Chatbot${NC} (opcional)"
echo ""
echo -e "${BLUE}Comandos √∫teis:${NC}"
echo -e "   ${YELLOW}pm2 status${NC}                    - Ver status de todos os servi√ßos"
echo -e "   ${YELLOW}pm2 logs zentria-backend${NC}     - Ver logs do backend"
echo -e "   ${YELLOW}pm2 logs zentria-front${NC}       - Ver logs do frontend"
echo -e "   ${YELLOW}pm2 restart zentria-backend${NC}  - Reiniciar backend"
echo -e "   ${YELLOW}pm2 restart zentria-front${NC}    - Reiniciar frontend"
echo ""
echo -e "${BLUE}Testar APIs:${NC}"
echo -e "   ${YELLOW}curl http://${SERVER_IP}:3001/api/health${NC}     - Backend API Health Check"
echo -e "   ${YELLOW}curl http://${SERVER_IP}:8080/instance/fetchInstances${NC} - Evolution API"
echo ""
success "Instala√ß√£o completa! Acesse http://${SERVER_IP}:5173 para come√ßar."
